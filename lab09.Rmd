---
title: "Lab 9"
author: "Steven Grgas"
output: pdf_document
date: "11:59PM May 2, 2020"
---

Set a seed and load the `adult` dataset and remove missingness. We also drop the education variable as it's linearly dependent with the education_num variable and will complicate the interactions further on.

```{r}
set.seed(1)
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult)
adult$education = NULL
```

We had problems with the features "occupation" and "native_country". Go through these two features and and identify levels with too few examples and wrap them into a level called "other". This is standard practice.

```{r}
sort(table(adult$occupation))
adult$occupation = as.character(adult$occupation)
adult$other = adult$occupation %in% c("Armed-Forces", "Priv-house-serv", "Protective-serv")
table(adult$other)
adult$occupation[adult$other] = "other"
adult$other = NULL
adult$occupation = as.factor(adult$occupation)
sort(table(adult$occupation))
sort(table(adult$native_country))
adult$domestic = ifelse(adult$native_country == "United-States", 1, 0)
table(adult$domestic)
adult$native_country = NULL
```


We will be doing model selection. We will split the dataset into 3 distinct subsets. Set the size of our splits here. For simplicitiy, all three splits will be identically sized. We are making it small so the stepwise algorithm can compute quickly. If you have a faster machine, feel free to increase this.

```{r}
Nsplitsize = 1000
```


Now create the following variables: `Xtrain`, `ytrain`, `Xselect`, `yselect`, `Xtest`, `ytest` with `Nsplitsize` observations:

```{r}
adult = adult[sample(1 : nrow(adult)), ]
Xtrain = adult[1 : Nsplitsize, ]
Xtrain$income = NULL
ytrain = ifelse(adult[1 : Nsplitsize, "income"] == ">50K", 1, 0)
Xselect = adult[(Nsplitsize + 1) : (2 * Nsplitsize), ]
Xselect$income = NULL
yselect = ifelse(adult[(Nsplitsize + 1) : (2 * Nsplitsize), "income"] ==">50K", 1, 0)
Xtest = adult[(2 * Nsplitsize + 1) : (3 * Nsplitsize), ]
Xtest$income = NULL
ytest = ifelse(adult[(2 * Nsplitsize + 1) : (3 * Nsplitsize), "income"] == ">50K", 1, 0)
```

Fit a vanilla logistic regression on the training set.

```{r}
logistic_mod = glm(ytrain ~ ., Xtrain, family = "binomial")
```

and report the log scoring rule, the Brier scoring rule.

```{r}
phat_train = predict(logistic_mod, Xtrain, type = 'response')
mean(ytrain * log(phat_train) + (1 - ytrain) * log(1 - phat_train)) #log score computation
mean(-(ytrain - phat_train)^2) #brier score computations
```


Then use this probability estimation model to do classification by thresholding at 0.5. Tabulate a confusion matrix and compute the misclassification error.

```{r}
y_hat_train = ifelse(phat_train >= 0.5, 1, 0)
table(ytrain, y_hat_train)
mean(ytrain != y_hat_train)
```

We will be doing model selection using a basis of linear features consisting of all interactions of the 14 raw features. Create a model matrix from the training data containing all these features. Make sure it has an intercept column too (the one vector is usually an important feature). Cast it as a data frame so we can use it more easily for modeling later on.

```{r}
Xmm_train = data.frame(model.matrix(~ . * ., Xtrain))
dim(Xmm_train)
```

We're going to need those model matrices (as data frames) for both the select and test sets. So make them here:

```{r}
Xmm_select = data.frame(model.matrix(~ . * . , Xselect))
dim(Xmm_select)
Xmm_test = data.frame(model.matrix(~ . * . , Xtest))
dim(Xmm_test)
```

Write code that will fit a model stepwise. You can refer to the chunk of line 83 in practice lecture 12. Use the Brier score to do the selection. Run the code and hit "stop" when you begin to the see the Brier score degrade appreciably oos. Be patient as it will wobble.

```{r}
pacman::p_load(Matrix)
p_plus_one = ncol(Xmm_train)
predictor_by_iteration = c() #keep a growing list of predictors by iteration
in_sample_brier_by_iteration = c() #keep a growing list of se's by iteration
oos_brier_by_iteration = c() #keep a growing list of se's by iteration
i = 1
repeat {
  #get all predictors left to try
  all_brier = array(NA, p_plus_one) #record all possibilities
  for (j_try in 1 : p_plus_one){
    if (!(j_try %in% predictor_by_iteration)){
      Xmm_sub = Xmm_train[, c(predictor_by_iteration, j_try), drop = FALSE]
      #we need a check here to ensure the matrix is full rank
      if (ncol(Xmm_sub) > rankMatrix(Xmm_sub)){
        next
      }
      #use suppressWarnings to get this to run without blasting the console
      logistic_mod = suppressWarnings(glm(ytrain ~ ., Xmm_sub, family = "binomial"))
      phatTrain = suppressWarnings(predict(logistic_mod, Xmm_sub, type = 'response'))
      all_brier[j_try] = mean(-(ytrain - phatTrain)^2) 
    }
  }
  j_star = which.max(all_brier) #We didn't catch this in lab... it has to be max Brier.
  predictor_by_iteration = c(predictor_by_iteration, j_star)
  in_sample_brier_by_iteration = c(in_sample_brier_by_iteration, all_brier[j_star])
  
  #now let's look at oos
  Xmm_sub = Xmm_train[, predictor_by_iteration, drop = FALSE]
  logistic_mod = suppressWarnings(glm(ytrain ~ ., Xmm_sub, family = "binomial"))
  phatTrain = suppressWarnings(predict(logistic_mod, Xmm_sub, type = 'response'))
  all_brier[j_try] = mean(-(ytrain - phatTrain)^2)
  
  phat_select = suppressWarnings(predict(logistic_mod, Xmm_select[, predictor_by_iteration, drop = FALSE], type = 'response'))
  oos_brier = mean(-(yselect - phat_select)^2) 
  oos_brier_by_iteration = c(oos_brier_by_iteration, oos_brier)
  
  cat("i =", i, "in sample brier = ", all_brier[j_star], "oos brier =", oos_brier, "\n   predictor added:", colnames(Xmm_train)[j_star], "\n")
  
  i = i + 1
  
  if (i > 5000 || i > p_plus_one){
    break #why??
  }
}
```

Plot the in-sample and oos (select set) Brier score by $p$. Does this look like what's expected?

```{r}
simulation_results = data.frame(
  iteration = 1 : length(in_sample_brier_by_iteration),
  in_sample_brier_by_iteration = in_sample_brier_by_iteration,
  oos_brier_by_iteration = oos_brier_by_iteration
)
pacman::p_load(latex2exp)
pacman::p_load(ggplot2)
ggplot(simulation_results) + 
  geom_line(aes(x = iteration, y = in_sample_brier_by_iteration), col = "red") +
  geom_line(aes(x = iteration, y = oos_brier_by_iteration), col = "blue") + 
  ylab(TeX("probabilistic error"))
```

Print out the coefficients of the model selection procedure's guess as to the locally optimal probability estimation model and interpret the five largest (in abolute value) coefficients. Do the signs make sense on these coefficients?

```{r}
max_brier_score = which.max(oos_brier_by_iteration)
predictor_by_iteration[1 : max_brier_score]
Xmm_sub = Xmm_train[colnames(Xmm_train)[predictor_by_iteration[1 : max_brier_score]]]
logistic_mod = suppressWarnings(glm(ytrain ~ ., Xmm_sub, family = "binomial"))
logistic_mod$coefficients
logistic_mod$coefficients[order(abs(logistic_mod$coefficients), decreasing = TRUE)[1 : 5]]
#For every increase in x, we increase the value of y using logistic model
```

Use this locally optimal probability estimation model to make predictions in all three data sets: train, select test. Compare to the Brier scores across all three sets. Is this expected?

```{r}
Xmm_select_sub = Xmm_select[colnames(Xmm_train)[predictor_by_iteration[1 : max_brier_score]]]
Xmm_test_sub = Xmm_test[colnames(Xmm_train)[predictor_by_iteration[1 : max_brier_score]]]
p_hat_train = predict(logistic_mod, Xmm_sub, type = "response")
p_hat_select = predict(logistic_mod, Xmm_select_sub, type = "response")
p_hat_test = predict(logistic_mod, Xmm_test_sub, type = "response")
brier_train = mean(-(ytrain - p_hat_train)^2)
brier_select = mean(-(yselect - p_hat_select)^2)
brier_test = mean(-(ytest - p_hat_test)^2)
```

Plot the probability predictions in the test set by `y`. Does this plot look good?

```{r}
ggplot(data.frame(p_hat_test = p_hat_test, ytest = factor(ytest))) + 
  geom_histogram(aes(x = p_hat_test, fill = ytest))
```

Calculate misclassification error, sensitivity (recall), specificity (true negative rate, TN / N), FDR, FOR for this model if you threshold at phat = 0.5. Interpret these metrics.

```{r}
threshold = ifelse(p_hat_test > 0.5, 1, 0)
positive = sum(ytest == 1)
negative = sum(ytest == 0)
true_positive = sum(ytest == 1 & threshold == 1)
true_negative = sum(ytest == 1 & threshold == 0)
false_positive = sum(ytest == 0 & threshold == 1)
false_negative = sum(ytest == 0 & threshold == 0)
```

Now, consider an asymmetric costs scenario. Let's say you're trying to sell people luxury products and want to advertise with only high-salaried individuals. Since your advertising is expensive, you want to not waste money on people who do not make a high salary. Thus your cost of predicting >50K when it truly is <=50K, i.e. a false positive (FP), is higher than predicting <=50K when the person truly makes >50K, i.e. a false negative (FN). Set the cost of FP to 3x more than the cost of FN. Use a grid of 0.001 to step through thresholds for the locally optimal probability estimation model (source the function from practice lecture 15). Do this in the selection dataset.

```{r}
#TO-DO
```

Plot an ROC curve for the selection dataset.

```{r}
#TO-DO
```

Calculate AUC and interpret.

```{r}
#TO-DO
```

Plot a DET curve for the selection dataset.

```{r}
#TO-DO
```

Calculate total cost for each classification model defined by each threshold.

```{r}
#TO-DO
```

Find the probability estimate threshold for the locally optimal asymmetric cost model for your FP and FN costs. Use this optimal probability estimate threshold and classify the test set. Print out its confusion matrix in the test set and calculate average cost per future observation, future FDR and future FOR and interpret these metrics in the context of this scenario. Is this model successful in internalizing your asymmetric costs?

```{r}
#TO-DO
```

Throughout the next part of this assignment you can use either the `tidyverse` package suite or `data.table` to answer but not base R. You can mix `data.table` with `magrittr` piping if you wish but don't go back and forth between `tbl_df`'s and `data.table` objects.

```{r}
pacman::p_load(tidyverse, magrittr, data.table)
```

We will be using the `storms` dataset from the `dplyr` package. Filter this dataset on all storms that have no missing measurements for the two diameter variables, "ts_diameter" and "hu_diameter".

```{r}
#TO-DO
```

From this subset, create a data frame that only has storm, observation period number (i.e., 1, 2, ..., T) and the "ts_diameter" and "hu_diameter" metrics.

```{r}
#TO-DO
```

Create a data frame in long format with columns "diameter" for the measurement and "diameter_type" which will be categorical taking on the values "hu" or "ts".

```{r}
#TO-DO
```

Using this long-formatted data frame, use a line plot to illustrate both "ts_diameter" and "hu_diameter" metrics by observation period for four random storms using a 2x2 faceting. The two diameters should appear in two different colors and there should be an appropriate legend.

```{r}
#TO-DO
```

---
title: "Lab 2"
author: "Steven Grgas"
output: pdf_document
date: "11:59PM February 17, 2020"
---

## More Basic R Skills


* Calculate the average of 1000 realizations of Bernoullis with p = 0.9 in one line using `rbinom`.

```{r}
mean(rbinom(1000, size = 1, prob = 0.9))
```

* In class we considered a variable `x_3` which measured "criminality". We imagined L = 4 levels "none", "infraction", "misdimeanor" and "felony". Create a variable `x3` here with 100 random elements (equally probable). Create it as a nominal (i.e. unordered) factor.

```{r}
x3 = factor(sample(c("none", "infraction", "misdimeanor", 
"felony"), size = 100, replace = TRUE))
x3
```

* Convert this variable into three binary variables without any information loss and put them into a data matrix.

```{r}
X = matrix(NA, nrow = length(x3), ncol = 3)
X[ ,1] = as.numeric(x3=="infraction")
X[ ,2] = as.numeric(x3=="felony")
X[ ,3] = as.numeric(x3=="misdimeanor")
colnames(X) = c("is_infraction", "is_felony", "is_misdimeanor")
X
```

* What should the sum of each row be (in English)? 

It should be either 1 or 0 because categories are mutually exclusive. We can only put an object into 1 category. None = 0

Verify that. 


```{r}
# rowSums(X)
table(rowSums(X))
```

* How should the column sum look (in English)? 

The sums should be around the expectation, 25 since they are uniformly distributed.

Verify that.


```{r}
colSums(X)
```

* Generate a matrix with 100 rows where the first column is realization from a normal with mean 17 and variance 38, the second column is uniform between -10 and 10, the third column is poisson with mean 6, the fourth column in exponential with lambda of 9, the fifth column is binomial with n = 20 and p = 0.12 and the sixth column is a binary variable with exactly 24% 1's dispersed randomly. Name the rows the entries of the `fake_first_names` vector.


```{r}
n = 100
X = matrix(data = NA, nrow = n, ncol = 6)
X[, 1] = rnorm(n = n, mean = 17, sd = sqrt(38))
X[, 2] = runif(n = n, min = -10, max = 10)
X[, 3] = rpois(n, lambda = 6)
X[, 4] = rexp(n, rate = 9)
X[, 5] = rbinom(n, size = 20, prob = 0.12)
X[, 6] = sample(c(rep(1, n * .24), rep(0, n * .76)))

fake_first_names = c(
  "Sophia", "Emma", "Olivia", "Ava", "Mia", "Isabella", "Riley", 
  "Aria", "Zoe", "Charlotte", "Lily", "Layla", "Amelia", "Emily", 
  "Madelyn", "Aubrey", "Adalyn", "Madison", "Chloe", "Harper", 
  "Abigail", "Aaliyah", "Avery", "Evelyn", "Kaylee", "Ella", "Ellie", 
  "Scarlett", "Arianna", "Hailey", "Nora", "Addison", "Brooklyn", 
  "Hannah", "Mila", "Leah", "Elizabeth", "Sarah", "Eliana", "Mackenzie", 
  "Peyton", "Maria", "Grace", "Adeline", "Elena", "Anna", "Victoria", 
  "Camilla", "Lillian", "Natalie", "Jackson", "Aiden", "Lucas", 
  "Liam", "Noah", "Ethan", "Mason", "Caden", "Oliver", "Elijah", 
  "Grayson", "Jacob", "Michael", "Benjamin", "Carter", "James", 
  "Jayden", "Logan", "Alexander", "Caleb", "Ryan", "Luke", "Daniel", 
  "Jack", "William", "Owen", "Gabriel", "Matthew", "Connor", "Jayce", 
  "Isaac", "Sebastian", "Henry", "Muhammad", "Cameron", "Wyatt", 
  "Dylan", "Nathan", "Nicholas", "Julian", "Eli", "Levi", "Isaiah", 
  "Landon", "David", "Christian", "Andrew", "Brayden", "John", 
  "Lincoln"
)

rownames(X) = fake_first_names
X
```

* Create a data frame of the same data as above except make the binary variable a factor "DOMESTIC" vs "FOREIGN" for 0 and 1 respectively. Print out the top few rows to check if this worked correctly.

```{r}
df = data.frame(X)
df$X6 = factor(df$X6, levels = c(0, 1), labels = c("DOMESTIC", "FOREIGN"))
df
```

* Print out a table of the binary variable. Then print out the proportions of "DOMESTIC" vs "FOREIGN".

```{r}
table(df$X6)
table(df$X6)/n
```
Print out a summary of the whole dataframe.

```{r}
summary(df)
```

* Let `n = 50`. Create a n x n matrix `R` of exactly 50% entries 0's, 25% 1's 25% 2's. These values should be in random locations.

```{r}
n = 50
X = matrix(sample(c(rep(0, n^2*.5),rep(1, n^2*.25), rep(2,n^2*.25))), nrow = n, ncol = n)
table(X)
```

* Randomly punch holes (i.e. `NA`) values in this matrix so that an each entry is missing with probability 30%.

```{r}
for(i in 1 : n){
  for(j in 1 : n) {
    if(runif(1) < 0.3) {
      X[i, j] = NA
    }
  }
}
sum(is.na(X)) / n^2
```

* Sort the rows in matrix `R` by the largest row sum to lowest. Be careful about the NA's!

```{r}
X[order(rowSums(X, na.rm = TRUE), decreasing = TRUE), ]
```

* We will now learn the `apply` function. This is a handy function that saves writing for loops which should be eschewed in R. Use the apply function to compute a vector whose entries are the standard deviation of each row. Use the apply function to compute a vector whose entries are the standard deviation of each column. Be careful about the NA's! This should be one line.

```{r}
matrix(c(apply(X, 1, sd, na.rm = TRUE), apply(X, 2, sd, na.rm = TRUE)), nrow = 50, ncol = 2)
```

* Use the `apply` function to compute a vector whose entries are the count of entries that are 1 or 2 in each column. This should be one line.

```{r}
apply(X, 2, function(v){sum((v == 1 | v == 2), na.rm = TRUE)})
```

* Use the `split` function to create a list whose keys are the column number and values are the vector of the columns. Look at the last example in the documentation `?split`.

```{r}
split(X, col(X))
```

* In one statement, use the `lapply` function to create a list whose keys are the column number and values are themselves a list with keys: "min" whose value is the minimum of the column, "max" whose value is the maximum of the column, "pct_missing" is the proportion of missingness in the column and "first_NA" whose value is the row number of the first time the NA appears.

```{r}
min_max_values = lapply(split(X, col(X), drop = TRUE),
        function(X){
          min_value = min(X, na.rm = TRUE)
          max_value = max(X, na.rm = TRUE)
          pct_missing = (sum(is.na(X)) / length(X)) * 100
          first_NA = min(which(is.na(X)))
          c(min_value, max_value, pct_missing, first_NA)
        })
min_max_values


```

* Create a vector `v` consisting of a sample of 1,000 iid normal realizations with mean -10 and variance 100.

```{r}
v <- sample(rnorm(1000, mean = -10, sd = 10)
v
```


* Create a function `my_reverse` which takes as required input a vector and returns the vector in reverse where the first entry is the last entry, etc. No function calls are allowed inside your function otherwise that would defeat the purpose of the exercise! (Yes, there is a base R function that does this called `rev`). Use `head` on `v` and `tail` on `my_reverse(v)` to verify it works.

```{r}
my_reverse <- function(vect){
  vect[length(vect):1]
}
v <- c(1, 2, 3, 4, 5, 6)
head(v, n=3)
tail(my_reverse(v), n = 3)

```


* Create a function `flip_matrix` which takes as required input a matrix, an argument `dim_to_rev` that returns the matrix with the rows in reverse order or the columns in reverse order depending on the `dim_to_rev` argument. Let the default be the dimension of the matrix that is greater.

```{r}

  flip_matrix = function(dim_to_rev){
  if(nrow(dim_to_rev) > ncol(dim_to_rev)){
    
    dim_to_rev[nrow(dim_to_rev):1, ]
  }else{
    dim_to_rev[ , ncol(dim_to_rev):1]
  }
}

      
```

* Find the average of `v` and the standard error of `v`.

```{r}
v <- c(1, 2, 3, 4, 5, 6)
mean(v)
sd(v)
```

* Find the 5%ile of `v` and use the `qnorm` function to compute what it theoretically should be. Is the estimate about what is expected by theory?

```{r}
v <- c(1, 2, 3, 4, 5, 6)
qnorm(0.05, mean(v), sd(v))
```

* What is the percentile of `v` that corresponds to the value 0? What should it be theoretically? Is the estimate about what is expected by theory?

```{r}
ecdf(0) ##the percentile of v that gives the value 0 is 0
```

* Create a list named `my_list` with keys "A", "B", ... where the entries are arrays of size 1, 2 x 2, 3 x 3 x 3, etc. Fill the array with the numbers 1, 2, 3, etc. Make 8 entries.


```{r}
my_list <- list(A = array(1, dim = c(1)), 
                B = array(1 : 2 ^ 2, dim = c(2, 2)),
                C = array(1 : 3 ^ 3, dim = c(3, 3, 3)),
                D = array(1 : 4 ^ 4, dim = c(4, 4, 4, 4)),
                E = array(1 : 5 ^ 5, dim = c(5, 5, 5, 5, 5)),
                F = array(1 : 6 ^ 6, dim = c(6, 6, 6, 6, 6, 6)),
                G = array(1 : 7 ^ 7, dim = c(7, 7, 7, 7, 7, 7, 7)),
                H = array(1 : 8 ^ 8, dim = c(8, 8, 8, 8, 8, 8, 8, 8))
                )
my_list
```

Run the following code:

```{r}
lapply(my_list, object.size)
```

Use `?object.size` to read about what these functions do. Then explain the output you see above. For the later arrays, does it make sense given the dimensions of the arrays?

?object.size
The bigger the arrays, the more data it takes to store them. Thats why the later arrays take exponentally more data to store them.

Now cleanup the namespace by deleting all stored objects and functions:

```{r}
rm(list = ls())
```

## A little about strings

* Use the `strsplit` function and `sample` to put the sentences in the string `lorem` below in random order. You will also need to manipulate the output of `strsplit` which is a list. You may need to learn basic concepts of regular expressions.

```{r}
lorem = "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi posuere varius volutpat. Morbi faucibus ligula id massa ultricies viverra. Donec vehicula sagittis nisi non semper. Donec at tempor erat. Integer dapibus mi lectus, eu posuere arcu ultricies in. Cras suscipit id nibh lacinia elementum. Curabitur est augue, congue eget quam in, scelerisque semper magna. Aenean nulla ante, iaculis sed vehicula ac, finibus vel arcu. Mauris at sodales augue."

lorem_random = sample(unlist(strsplit(lorem, "[.]")))
paste(lorem_random, collapse =".")
```




